{\rtf1\ansi\ansicpg1252\cocoartf1348\cocoasubrtf170
{\fonttbl\f0\fswiss\fcharset0 ArialMT;\f1\fnil\fcharset0 Menlo-Regular;}
{\colortbl;\red255\green255\blue255;\red38\green38\blue38;\red255\green255\blue255;\red53\green118\blue190;
\red51\green51\blue51;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid1}
{\list\listtemplateid2\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid101\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid2}
{\list\listtemplateid3\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid201\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid3}
{\list\listtemplateid4\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid301\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid4}
{\list\listtemplateid5\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid401\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid5}
{\list\listtemplateid6\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid501\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid6}
{\list\listtemplateid7\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid601\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid7}
{\list\listtemplateid8\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid701\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid8}
{\list\listtemplateid9\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid801\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid9}
{\list\listtemplateid10\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid901\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid10}
{\list\listtemplateid11\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1001\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid11}
{\list\listtemplateid12\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1101\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid12}
{\list\listtemplateid13\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1201\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid13}
{\list\listtemplateid14\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1301\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid14}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}{\listoverride\listid2\listoverridecount0\ls2}{\listoverride\listid3\listoverridecount0\ls3}{\listoverride\listid4\listoverridecount0\ls4}{\listoverride\listid5\listoverridecount0\ls5}{\listoverride\listid6\listoverridecount0\ls6}{\listoverride\listid7\listoverridecount0\ls7}{\listoverride\listid8\listoverridecount0\ls8}{\listoverride\listid9\listoverridecount0\ls9}{\listoverride\listid10\listoverridecount0\ls10}{\listoverride\listid11\listoverridecount0\ls11}{\listoverride\listid12\listoverridecount0\ls12}{\listoverride\listid13\listoverridecount0\ls13}{\listoverride\listid14\listoverridecount0\ls14}}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\sl480\sa320

\f0\fs44 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Regression Week 5: LASSO Assignment 1\
\pard\pardeftab720\sl420\sa280

\fs28 \cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 In this assignment, you will use LASSO to select features, building on a pre-implemented solver for LASSO (using GraphLab Create, though you can use other solvers). You will:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl420\sa280
\ls1\ilvl0\cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\'95	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Run LASSO with different L1 penalties.\cb1 \
\ls1\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\'95	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Choose best L1 penalty using a validation set.\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl420
\ls1\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\'95	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Choose best L1 penalty using a validation set, with additional constraint on the size of subset.\cb1 \
\pard\pardeftab720\sl420\sa280
\cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 In the second assignment, you will implement your own LASSO solver, using coordinate descent.\
\pard\pardeftab720\sl480\sa320

\fs44 \cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 IMPORTANT: Choice of tools\
\pard\pardeftab720\sl420\sa280

\b\fs28 \cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 For the purpose of this assessment, you may choose between GraphLab Create and scikit-learn (with Pandas). You are free to experiment with other tools (e.g. R or Matlab), but they may not produce correct numbers for the quiz questions.
\b0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl420\sa280
\ls2\ilvl0\cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\'95	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 If you are using GraphLab Create, download the IPython notebook and follow the instructions contained in the notebook.\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl420
\ls2\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\'95	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 If you are using Pandas+scikit-learn combination, follow through the instructions in this reading.\cb1 \
\pard\pardeftab720\sl480\sa320

\fs44 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 What you need to download\
\pard\pardeftab720\sl480\sa240

\fs40 \cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 If you are using GraphLab Create:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl420\sa280
\ls3\ilvl0
\fs28 \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\'95	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Download the King County House Sales data In SFrame format: {\field{\*\fldinst{HYPERLINK "https://eventing.coursera.org/api/redirectStrict/KvOr341PpKAP9B0jhJE9btcD-vgZcPYTrXVdJ6fDk0JEhjjBiTmS3Tz5HVUKbQt2WE23dA9QNfwO24oWK8V2jQ.bVbZvFYxcMVn1Zqa0DqOrQ.IlMOXHRzb1SiU6y6KO0ybXosVO0qIN1CNspTLrbrZ5T3SWSc0F76nD8Aha0GjEGf85AH7Et66ojhPtjBygZYpyxX24xD1yy4ZK2bucXSjJOuH3ipdwUjQLRrRV0TVqL-FFNidlISRKE0MlRFslhZ4vaNpaf2wQkPeKcMi7Bp9NbB5Zz0pCSIX-bOa2Mh2XnL9crj4el6d7ToXYGmNqXWCwd_i-v3kY5a-p5FmeQpnoFbKp9TOVX_wwW9aELZmHi0CJ76F42arXGyAcxCtgZYD0W9xliOeCeFqYhyqq0Kxx5EHpg4AY_at8OidfQCNpCW5Iy8RilK-ZhHa38Z50MtXCdm0uTz0p5d_HiO28ugNr8FRFDDAy_lFyv_jIHz3wN0KU8m9k8Ph6Ry77UPE1xIhWbBvnkVOjPFE7CNgiCphM7lU7euzZKrmKUhbIMjTaLg"}}{\fldrslt \cf4 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 kc_house_data.gl.zip}}\cb1 \
\ls3\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\'95	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Download the companion IPython Notebook: {\field{\*\fldinst{HYPERLINK "https://eventing.coursera.org/api/redirectStrict/lVcV4vhT7JlqEhx_rbUfUNK69wpJCrMAfHv-ef-lp2IdrA8X0AneY0STxWLyt4KW7i-KH0BUJ9F1TeioO_8QJg.GT8xs6MWWkh8nkOG7RHuYQ.hJo7ba_W9NZcB3nR2YSow2I0vdiVQVq_5JmxiUrbyf-iL31XmYpPsTNLqSRyH_0-Sk7iCT3NTe_1XOSc5UiNiTQn5yBY1uX_-geNhNOoJR2NHxX3o_DTrXSRnggRJ106j0-9tgXCCOOY0-k8iR-OBuvrypiu6Iv3LSB6Tze4q_g5b5VVOGpx_L6jvX-dajqqX3Fuadqeh_ms5dNS98djrSnFidndKaY8XV7PBbl6FK_bbVjaB8E4Wgs0yyEER3uhJdHWTdghRKfo5A8bQnOHrdA3RGOxNBCP1L5GHk26phDNQxCXJsrenCbQSvXHNkPGjoLGPFgFB1s_w0G0ehLWTtjxF8O6bvVpxaRmdItTktWkGG1-7q6oTW9yj8roD563Wp1u7b60Y71IFnKKKBtOR8d29t-RJXkNfZMFgJV_5mOZwDi-EzG80sM7UHwrRKNN_oZzLSfmzV4XpLl0yzTusDIo51GJHeQ5TVtMNLm0t3c"}}{\fldrslt \cf4 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 week-5-lasso-assignment-1-blank.ipynb}}\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl420
\ls3\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\'95	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Save both of these files in the same directory (where you are calling IPython notebook from) and unzip the data file.\cb1 \
\pard\pardeftab720\sl480\sa240

\fs40 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 If you are using scikit-learn with Pandas:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl420
\ls4\ilvl0
\fs28 \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\'95	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Download the King County House Sales data csv file: {\field{\*\fldinst{HYPERLINK "https://eventing.coursera.org/api/redirectStrict/JFYknFnS6uCZdCppx4LQOSTiiS0pFVNTWve7GjB_707gf9r2B9wlCv6SaF1F3x1r0eAmZ-jYPKazexV-f9N3tA.3SLjL9w_KSRn7JGa2b-x1w.RrsjHliFpmE4aVWJR3pysNLD5LYNWsLOY72maMIHpep9ha10PdjPotL54FPQsAFoibtId8O7OLgCpJZINnRrVx1AexqatDI6QzuHOmIEzjGbWWGXUUdsRTMtc8KvVsFKD77uNPt7RprTFPf76hCMsm5QY_hEmwhWkO_IRhxVo0n49AfPZI9rLcnzb4neIvGHEZ5VnSi8ZomJU-UxYylO2ogQJDDB5tzpg0nIencwfrfH-yUAvsPRabeJU5oSyRfAR4XZ4a1MZLrt0W1_WBBbKjxcSY0kvuvL-s3lm5bd-5RkJa0OyjY6SMJUr5Us1-BYIRWPg7orvXz_P8E-73jnGgp_EsINDoZPxYXOTxnEr3Pev3hOtKt0uqH8uH8tFPqSUMTMsn6NAyOIXWQ9j1XiPrYr4eIsFsf9cj7evRHY94p52vFTDLZFAs7ju0IzW2Yc"}}{\fldrslt \cf4 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 kc_house_data.csv}}\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl420\sa280
\ls5\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\'95	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Download the King County House Sales training data csv file: {\field{\*\fldinst{HYPERLINK "https://eventing.coursera.org/api/redirectStrict/DHiISEOa0al2NC6w_VS-rFfabXoVylebPoPb6LHR-PAiJJVRuDG1FWsRNbxNMpL5oyXpnZ0ud7Wpfc4pG_pO4Q.-Znxp4GuIcqSoFMieVvOkw.thyfgscfJ3CZdqFa56rmQKo3UnZUV501C1Zf6IUuCcxllhpKz750DrXb-M2BsakPBzttGsFD8tYEW6MENxeSUHkHFRGPErYc9_GtyfkapySkMUQHZ9xy-UDzjJ-6azkwETW-EREdhQ3883J9pg3yNqNNymklXIO9fa1oTkXOpPTyHC28tPzOr40X4mcp-mHDlCY1Y_s-D6P_C1uZQqryb-fcWGi_aUTA_hhgfwp-uHEsbGR4x6Lf7VDG_PQoAxVFh95CBTy-Q170Dpi7r9SvSgXeFLsGmOMXfpfuokMSakQoKCTpz9LsnLnNLgcLcgzd5-c3voh0oEWkcBRkbR7oew_Z-0fcm8sxoRV84iUdxb-Jy7d5KJFB6Eu07wDyqrfyTCocQ6W7Sue7kFXznqJto8bUyFSD9Aw3akBr1PCXSrzGVpXSMs04aSQEJfC6-0hNWVz1iJN38U7Ad5p3PCB5Zg"}}{\fldrslt \cf4 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 wk3_kc_house_train_data.csv}}\cb1 \
\ls5\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\'95	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Download the King County House Sales validation data csv file: {\field{\*\fldinst{HYPERLINK "https://eventing.coursera.org/api/redirectStrict/qyGhjRS-BGyj20InPoHxEQRScKLAA1VwtUmJHzoItB8cwXLysw-x59a7qjSQ9iZbnG6s0eBlD4cV8dxJYnkmCw.2rtdR8z_rL4T8KB1RCR3-w.6G8gv3OngG_mBGoNK3YvXn3AeEiNDLg_-tF13tXNW_s5ZE7jQVb33LJ7PauQ6Q9O6r1_w8qen7mfRq4n3o0Ikc86WQbBej36MbuWEpZ8pCHKoQleVcOfGGPPkSEJ6fY64FVhy4rZxcSFmKiVavtlSZNhYizZY9RNGpPoTt7MyXUypizZfZynXXwahSKKc7P7zAjCJ9LUC4V9TNMCg7FgfGpwKEmQ7e47Jr1hMjeJpzweqeq4fy4Hp69fWXSHIKEbFSpWgA2F9tE9ucUC6TXruR1Sbx67D_Kw9cS7Ja1B2DsBLwfSyTcMc-8LSwg0ekujVxyWpiwAwHudQDNyLvgI8PVJX-X_3T6S2zkrABYAVkegElzb9aeLiGE7157ULEK-mbsmf-43-pZnqhk1Bm_hNiWb_iK4ssRaBd7CqEj_HNtPBVUZ8PFeEQ7B9jPVzHU_xsn1OjMj2PoaQvhJP8OFjQ"}}{\fldrslt \cf4 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 wk3_kc_house_valid_data.csv}}\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl420
\ls5\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\'95	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Download the King County House Sales testing data csv file: {\field{\*\fldinst{HYPERLINK "https://eventing.coursera.org/api/redirectStrict/1A9psRh0zOmzLu0W5GrzF3j8d0Hc-Rjj_gOF_OIxLjfvQOwLMReNaQko4h-s3-2FBJWM9kGXbk9eaT5JqVi1Pg.R99Xeid_PUgnI-0aGtXIgw.4idmDqchZ2HtFEUshdRCwwmgLgCdv-g0EeIiChYz8L18pC4tAR-NfTKlxG0xVv8c7rJGxWxdXeHGdb_1eNzoiwj3Vis8gn3KbCuwgAazZ52TI50XVNyeW8Ya9uIHMB2C6bdURpPis-vnN9RwXT-NovmDLhYD2mGq3CCi2k5bvAG_A_4k1R-ASSARM15q5upd1f0lXB_7b-ICi1jd9lY6QhjXjIuLXkm9Mige5LzNGAQYA8Tmfrwa_9hz_Ngq-AFeTxYSmx5SmKCT4_bA6HFbRMXPUfiVeZYuT-PT2-wKPaX75qIBt4YpCrrM1PtozVINZlZ6g2JZ7OI5GRaV8V0s6yb97-0zY6SS8Dr9pCODi2KakjpvGosmlajFamMdufvVSxdVOpUEuFeygURy879dx2CWHzN13c0l88grI5DKpQT2jsb6Wp0ykgfdLx21EBhrsn8r8SNuiMoCro_j27Ltvw"}}{\fldrslt \cf4 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 wk3_kc_house_test_data.csv}}\cb1 \
\pard\pardeftab720\sl480\sa320

\fs44 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 Useful resources\
\pard\pardeftab720\sl420\sa280

\fs28 \cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 You may need to install the software tools or use the free Amazon EC2 machine. Instructions for both options are provided in the reading for Module 1 (Simple Regression).\
If you are following the IPython Notebook and/or are new to numpy then you might find the following tutorial helpful: numpy-tutorial.ipynb\
\pard\pardeftab720\sl480\sa320

\fs44 \cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 If you are using GraphLab Create and the companion IPython Notebook\
\pard\pardeftab720\sl420\sa280

\fs28 \cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 Open the companion IPython notebook and follow the instructions in the notebook.\
\pard\pardeftab720\sl480\sa320

\fs44 \cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 If you are using scikit-learn with Pandas:\
\pard\pardeftab720\sl420\sa280

\fs28 \cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 The instructions may apply to other tools, but the set of parameters are specific to scikit-learn.\
\pard\pardeftab720\sl420\sa280

\b \cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 0
\b0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 . Load the sales dataset using Pandas:\
\pard\pardeftab720\sl360

\f1\fs26 \cf3 \cb5 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec3 import pandas as pd\
\
dtype_dict = \{'bathrooms':float, 'waterfront':int, 'sqft_above':int, 'sqft_living15':float, 'grade':int, 'yr_renovated':int, 'price':float, 'bedrooms':float, 'zipcode':str, 'long':float, 'sqft_lot15':float, 'sqft_living':float, 'floors':float, 'condition':int, 'lat':float, 'date':str, 'sqft_basement':int, 'yr_built':int, 'id':str, 'sqft_lot':int, 'view':int\}\
\
sales = pd.read_csv('kc_house_data.csv', dtype=dtype_dict)\
\pard\pardeftab720\sl420\sa280

\f0\b\fs28 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 1. 
\b0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 Create new features by performing following transformation on inputs:\
\pard\pardeftab720\sl360

\f1\fs26 \cf3 \cb5 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec3 from math import log, sqrt\
sales['sqft_living_sqrt'] = sales['sqft_living'].apply(sqrt)\
sales['sqft_lot_sqrt'] = sales['sqft_lot'].apply(sqrt)\
sales['bedrooms_square'] = sales['bedrooms']*sales['bedrooms']\
sales['floors_square'] = sales['floors']*sales['floors']\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl420\sa280
\ls6\ilvl0
\f0\fs28 \cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\'95	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Squaring bedrooms will increase the separation between not many bedrooms (e.g. 1) and lots of bedrooms (e.g. 4) since 1^2 = 1 but 4^2 = 16. Consequently this variable will mostly affect houses with many bedrooms.\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl420
\ls6\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\'95	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 On the other hand, taking square root of sqft_living will decrease the separation between big house and small house. The owner may not be exactly twice as happy for getting a house that is twice as big.\cb1 \
\pard\pardeftab720\sl420\sa280

\b \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 2. 
\b0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 Using the entire house dataset, learn regression weights using an L1 penalty of 5e2. Make sure to add "normalize=True" when creating the Lasso object. Refer to the following code snippet:\
\pard\pardeftab720\sl360

\f1\fs26 \cf3 \cb5 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec3 from sklearn import linear_model  # using scikit-learn\
\
model_all = linear_model.Lasso(alpha=5e2, normalize=True) # set parameters\
model_all.fit(sales[all_features], sales['price']) # learn weights\
\pard\pardeftab720\sl420\sa280

\f0\b\fs28 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 3. Quiz Question: Which features have been chosen by LASSO, i.e. which features were assigned nonzero weights?
\b0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \

\b \expnd0\expndtw0\kerning0
\outl0\strokewidth0 4. 
\b0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 To find a good L1 penalty, we will explore multiple values using a validation set. Let us do three way split into train, validation, and test sets. Download the provided csv files containing training, validation and test sets.\
\pard\pardeftab720\sl360

\f1\fs26 \cf3 \cb5 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec3 testing = pd.read_csv('wk3_kc_house_test_data.csv', dtype=dtype_dict)\
training = pd.read_csv('wk3_kc_house_train_data.csv', dtype=dtype_dict)\
validation = pd.read_csv('wk3_kc_house_valid_data.csv', dtype=dtype_dict)\
\pard\pardeftab720\sl420\sa280

\f0\fs28 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Make sure to create the 4 features as we did in #1:\
\pard\pardeftab720\sl360

\f1\fs26 \cf3 \cb5 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec3 testing['sqft_living_sqrt'] = testing['sqft_living'].apply(sqrt)\
testing['sqft_lot_sqrt'] = testing['sqft_lot'].apply(sqrt)\
testing['bedrooms_square'] = testing['bedrooms']*testing['bedrooms']\
testing['floors_square'] = testing['floors']*testing['floors']\
\
training['sqft_living_sqrt'] = training['sqft_living'].apply(sqrt)\
training['sqft_lot_sqrt'] = training['sqft_lot'].apply(sqrt)\
training['bedrooms_square'] = training['bedrooms']*training['bedrooms']\
training['floors_square'] = training['floors']*training['floors']\
\
validation['sqft_living_sqrt'] = validation['sqft_living'].apply(sqrt)\
validation['sqft_lot_sqrt'] = validation['sqft_lot'].apply(sqrt)\
validation['bedrooms_square'] = validation['bedrooms']*validation['bedrooms']\
validation['floors_square'] = validation['floors']*validation['floors']\
\pard\pardeftab720\sl420\sa280

\f0\b\fs28 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 5. 
\b0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 Now for each l1_penalty in [10^1, 10^1.5, 10^2, 10^2.5, ..., 10^7] (to get this in Python, type np.logspace(1, 7, num=13).)\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl420
\ls7\ilvl0\cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\'95	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Learn a model on TRAINING data using the specified l1_penalty. Make sure to specify normalize=True in the constructor:\cb1 \
\pard\pardeftab720\sl360

\f1\fs26 \cf3 \cb5 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec3 model = linear_model.Lasso(alpha=l1_penalty, normalize=True)\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl420
\ls8\ilvl0
\f0\fs28 \cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\'95	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Compute the RSS on VALIDATION for the current model (print or save the RSS)\cb1 \
\pard\pardeftab720\sl420\sa280
\cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 Report which L1 penalty produced the lower RSS on VALIDATION.\
\pard\pardeftab720\sl420\sa280

\b \cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 6. Quiz Question: Which was the best value for the l1_penalty, i.e. which value of l1_penalty produced the lowest RSS on VALIDATION data?
\b0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \
7. Now that you have selected an L1 penalty, compute the RSS on TEST data for the model with the best L1 penalty.\

\b \expnd0\expndtw0\kerning0
\outl0\strokewidth0 8. Quiz Question: Using the best L1 penalty, how many nonzero weights do you have? Count the number of nonzero coefficients first, and add 1 if the intercept is also nonzero. 
\b0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 A succinct way to do this is\
\pard\pardeftab720\sl360

\f1\fs26 \cf3 \cb5 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec3 np.count_nonzero(model.coef_) + np.count_nonzero(model.intercept_)\
\pard\pardeftab720\sl420\sa280

\f0\fs28 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 where 'model' is an instance of linear_model.Lasso.\
\pard\pardeftab720\sl420\sa280

\b \cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 9. 
\b0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 What if we absolutely wanted to limit ourselves to, say, 7 features? This may be important if we want to derive "a rule of thumb" --- an interpretable model that has only a few features in them.\
You are going to implement a simple, two phase procedure to achieve this goal:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl420\sa280
\ls9\ilvl0\cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\'95	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Explore a large range of \'91l1_penalty\'92 values to find a narrow region of \'91l1_penalty\'92 values where models are likely to have the desired number of non-zero weights.\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl420
\ls9\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\'95	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Further explore the narrow region you found to find a good value for \'91l1_penalty\'92 that achieves the desired sparsity. Here, we will again use a validation set to choose the best value for \'91l1_penalty\'92.\cb1 \
\pard\pardeftab720\sl420\sa280

\b \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 10. 
\b0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 Assign 7 to the variable \'91max_nonzeros\'92.\

\b \expnd0\expndtw0\kerning0
\outl0\strokewidth0 11. 
\b0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 Exploring large range of l1_penalty\
For l1_penalty in np.logspace(1, 4, num=20):\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl420
\ls10\ilvl0\cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\'95	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Fit a regression model with a given l1_penalty on TRAIN data. Add "alpha=l1_penalty" and "normalize=True" to the parameter list.\cb1 \
\pard\pardeftab720\sl360

\f1\fs26 \cf3 \cb5 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec3 model = linear_model.Lasso(alpha=l1_penalty, normalize=True)\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl420
\ls11\ilvl0
\f0\fs28 \cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\'95	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Extract the weights of the model and count the number of nonzeros. Take account of the intercept as we did in #8, adding 1 whenever the intercept is nonzero. Save the number of nonzeros to a list.\cb1 \
\pard\pardeftab720\sl420\sa280

\b \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 12. 
\b0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 Out of this large range, we want to find the two ends of our desired narrow range of l1_penalty. At one end, we will have l1_penalty values that have too few non-zeros, and at the other end, we will have an l1_penalty that has too many non-zeros.\
More formally, find:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl420\sa280
\ls12\ilvl0\cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\'95	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 The largest l1_penalty that has more non-zeros than \'91max_nonzeros\'92 (if we pick a penalty smaller than this value, we will definitely have too many non-zero weights)Store this value in the variable \'91l1_penalty_min\'92 (we will use it later)\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl420
\ls12\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\'95	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 The smallest l1_penalty that has fewer non-zeros than \'91max_nonzeros\'92 (if we pick a penalty larger than this value, we will definitely have too few non-zero weights)Store this value in the variable \'91l1_penalty_max\'92 (we will use it later)\cb1 \
\pard\pardeftab720\sl420\sa280
\cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 Hint: there are many ways to do this, e.g.:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl420\sa280
\ls13\ilvl0\cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\'95	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Programmatically within the loop above\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl420
\ls13\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\'95	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Creating a list with the number of non-zeros for each value of l1_penalty and inspecting it to find the appropriate boundaries.\cb1 \
\pard\pardeftab720\sl420\sa280

\b \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 13. Quiz Question: What values did you find for l1_penalty_min and l1_penalty_max?
\b0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \

\b \expnd0\expndtw0\kerning0
\outl0\strokewidth0 14. 
\b0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 Exploring narrower range of l1_penalty\
We now explore the region of l1_penalty we found: between \'91l1_penalty_min\'92 and \'91l1_penalty_max\'92. We look for the L1 penalty in this range that produces exactly the right number of nonzeros and also minimizes RSS on the VALIDATION set.\
For l1_penalty in np.linspace(l1_penalty_min,l1_penalty_max,20):\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl420\sa280
\ls14\ilvl0\cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\'95	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Fit a regression model with a given l1_penalty on TRAIN data. As before, use "alpha=l1_penalty" and "normalize=True".\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl420
\ls14\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\'95	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Measure the RSS of the learned model on the VALIDATION set\cb1 \
\pard\pardeftab720\sl420\sa280
\cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 Find the model that the lowest RSS on the VALIDATION set and has sparsity equal to \'91max_nonzeros\'92. (Again, take account of the intercept when counting the number of nonzeros.)\
\pard\pardeftab720\sl420\sa280

\b \cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 15. Quiz Question: What value of l1_penalty in our narrow range has the lowest RSS on the VALIDATION set and has sparsity equal to \'91max_nonzeros\'92?
\b0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \
\pard\pardeftab720\sl420

\b \cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 16. Quiz Question: What features in this model have non-zero coefficients?
\b0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \
}